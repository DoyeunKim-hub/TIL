{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 라이브러리 및 데이터 불러오기\n",
    "# 필요한 라이브러리를 불러온다.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "\n",
    "# 데이터 전처리 방식을 지정한다.\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), # 데이터를 PyTorch의 Tensor 형식으로 바꾼다.\n",
    "        transforms.Normalize(mean=(0.5,), std=(0.5,)) # 픽셀값 0 ~ 1 -> -1 ~ 1\n",
    "])\n",
    "\n",
    "# MNIST 데이터셋을 불러온다. 지정한 폴더에 없을 경우 자동으로 다운로드한다.\n",
    "mnist = datasets.MNIST(root='data', download=True, transform=transform)\n",
    "\n",
    "# 데이터를 한번에 batch_size만큼만 가져오는 dataloader를 만든다.\n",
    "dataloader = DataLoader(mnist, batch_size=60, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import imageio\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    use_gpu = True\n",
    "leave_log = True\n",
    "\n",
    "if leave_log:\n",
    "    result_dir = 'GAN_generated_images'\n",
    "    \n",
    "    if not os.path.isdir(result_dir):\n",
    "        os.mkdir(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GAN의 생성자(Generator)\n",
    "# 생성자는 랜덤 벡터 z를 입력으로 받아 가짜 이미지를 출력한다.\n",
    "class Generator(nn.Module):\n",
    "\n",
    "    # 네트워크 구조\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=100, out_features=256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(in_features=1024, out_features=28*28),\n",
    "            nn.Tanh())\n",
    "    \n",
    "    # (batch_size x 100) 크기의 랜덤 벡터를 받아 \n",
    "    # 이미지를 (batch_size x 1 x 28 x 28) 크기로 출력한다.\n",
    "    def forward(self, inputs):\n",
    "        return self.main(inputs).view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GAN의 구분자(Discriminator)\n",
    "# 구분자는 이미지를 입력으로 받아 이미지가 진짜인지 가짜인지 출력한다.\n",
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    # 네트워크 구조\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(in_features=28*28, out_features=1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(inplace=True),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=1),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    # (batch_size x 1 x 28 x 28) 크기의 이미지를 받아\n",
    "    # 이미지가 진짜일 확률을 0~1 사이로 출력한다.\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view(-1, 28*28)\n",
    "        return self.main(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 생성자와 구분자 객체 만들기\n",
    "\n",
    "G = Generator()\n",
    "D = Discriminator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 손실 함수와 최적화 기법 지정하기\n",
    "# Binary Cross Entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 생성자의 매개 변수를 최적화하는 Adam optimizer\n",
    "G_optimizer = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# 구분자의 매개 변수를 최적화하는 Adam optimizer\n",
    "D_optimizer = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 시각화하기\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def square_plot(data, path):\n",
    "    \"\"\"Take an array of shape (n, height, width) or (n, height, width , 3)\n",
    "       and visualize each (height, width) thing in a grid of size approx. sqrt(n) by sqrt(n)\"\"\"\n",
    "\n",
    "    if type(data) == list:\n",
    "\t    data = np.concatenate(data)\n",
    "    # normalize data for display\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "    # force the number of filters to be square\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "\n",
    "    padding = (((0, n ** 2 - data.shape[0]) ,\n",
    "                (0, 1), (0, 1))  # add some space between filters\n",
    "               + ((0, 0),) * (data.ndim - 3))  # don't pad the last dimension (if there is one)\n",
    "    data = np.pad(data , padding, mode='constant' , constant_values=1)  # pad with ones (white)\n",
    "\n",
    "    # tilethe filters into an image\n",
    "    data = data.reshape((n , n) + data.shape[1:]).transpose((0 , 2 , 1 , 3) + tuple(range(4 , data.ndim + 1)))\n",
    "\n",
    "    data = data.reshape((n * data.shape[1] , n * data.shape[3]) + data.shape[4:])\n",
    "\n",
    "    plt.imsave(path, data, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\doyeu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "if leave_log:\n",
    "    train_hist = {}\n",
    "    train_hist['D_losses'] = []\n",
    "    train_hist['G_losses'] = []\n",
    "    generated_images = []\n",
    "    \n",
    "z_fixed = Variable(torch.randn(5 * 5, 100), volatile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [60, 256]], which is output 0 of LeakyReluBackward1, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ec79f80e02b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mD_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;31m# 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mD_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \"\"\"\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [60, 256]], which is output 0 of LeakyReluBackward1, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "### 모델 학습을 위한 반복문\n",
    "# 데이터셋을 100번 돌며 학습한다.\n",
    "for epoch in range(100):\n",
    "    \n",
    "    if leave_log:\n",
    "        D_losses = []\n",
    "        G_losses = []\n",
    "    \n",
    "    # 한번에 batch_size만큼 데이터를 가져온다.\n",
    "    for real_data, _ in dataloader:\n",
    "        batch_size = real_data.size(0)\n",
    "        \n",
    "        # 데이터를 pytorch의 변수로 변환한다.\n",
    "        real_data = Variable(real_data)\n",
    "\n",
    "        ### 구분자 학습시키기\n",
    "\n",
    "        # 이미지가 진짜일 때 정답 값은 1이고 가짜일 때는 0이다.\n",
    "        # 정답지에 해당하는 변수를 만든다.\n",
    "        target_real = Variable(torch.ones(batch_size, 1))\n",
    "        target_fake = Variable(torch.zeros(batch_size, 1))\n",
    "         \n",
    "        \n",
    "            \n",
    "        # 진짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_real = D(real_data)\n",
    "        # 구분자의 출력값이 정답지인 1에서 멀수록 loss가 높아진다.\n",
    "        D_loss_real = criterion(D_result_from_real, target_real)\n",
    "\n",
    "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
    "        z = Variable(torch.randn((batch_size, 100)))\n",
    "        \n",
    "        \n",
    "            \n",
    "        # 생성자로 가짜 이미지를 생성한다.\n",
    "        fake_data = G(z)\n",
    "        \n",
    "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_fake = D(fake_data)\n",
    "        # 구분자의 출력값이 정답지인 0에서 멀수록 loss가 높아진다.\n",
    "        D_loss_fake = criterion(D_result_from_fake, target_fake)\n",
    "        \n",
    "        # 구분자의 loss는 두 문제에서 계산된 loss의 합이다.\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        \n",
    "        # 구분자의 매개 변수의 미분값을 0으로 초기화한다.\n",
    "        D.zero_grad()\n",
    "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
    "        D_loss.backward()\n",
    "        # 최적화 기법을 이용해 구분자의 매개 변수를 업데이트한다.\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        if leave_log:\n",
    "            D_losses.append(D_loss.data[0])\n",
    "\n",
    "        # train generator G\n",
    "\n",
    "        ### 생성자 학습시키기\n",
    "        \n",
    "        # 생성자에 입력으로 줄 랜덤 벡터 z를 만든다.\n",
    "        z = Variable(torch.randn((batch_size, 100)))\n",
    "        \n",
    "        if use_gpu:\n",
    "            z = z.cuda()\n",
    "        \n",
    "        # 생성자로 가짜 이미지를 생성한다.\n",
    "        fake_data = G(z)\n",
    "        # 생성자가 만든 가짜 이미지를 구분자에 넣는다.\n",
    "        D_result_from_fake = D(fake_data)\n",
    "        # 생성자의 입장에서 구분자의 출력값이 1에서 멀수록 loss가 높아진다.\n",
    "        G_loss = criterion(D_result_from_fake, target_real)\n",
    "        \n",
    "        # 생성자의 매개 변수의 미분값을 0으로 초기화한다.\n",
    "        G.zero_grad()\n",
    "        # 역전파를 통해 매개 변수의 loss에 대한 미분값을 계산한다.\n",
    "        G_loss.backward()\n",
    "        # 최적화 기법을 이용해 생성자의 매개 변수를 업데이트한다.\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        if leave_log:\n",
    "            G_losses.append(G_loss.data[0])\n",
    "    if leave_log:\n",
    "        true_positive_rate = (D_result_from_real > 0.5).float().mean().data[0]\n",
    "        true_negative_rate = (D_result_from_fake < 0.5).float().mean().data[0]\n",
    "        base_message = (\"Epoch: {epoch:<3d} D Loss: {d_loss:<8.6} G Loss: {g_loss:<8.6} \"\n",
    "                        \"True Positive Rate: {tpr:<5.1%} True Negative Rate: {tnr:<5.1%}\"\n",
    "                       )\n",
    "        message = base_message.format(\n",
    "                    epoch=epoch,\n",
    "                    d_loss=sum(D_losses)/len(D_losses),\n",
    "                    g_loss=sum(G_losses)/len(G_losses),\n",
    "                    tpr=true_positive_rate,\n",
    "                    tnr=true_negative_rate\n",
    "        )\n",
    "        print(message)\n",
    "    \n",
    "    if leave_log:\n",
    "        fake_data_fixed = G(z_fixed)\n",
    "        image_path = result_dir + '/epoch{}.png'.format(epoch)\n",
    "        square_plot(fake_data_fixed.view(25, 28, 28).cpu().data.numpy(), path=image_path)\n",
    "        generated_images.append(image_path)\n",
    "    \n",
    "    if leave_log:\n",
    "        train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
    "        train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n",
    "\n",
    "torch.save(G.state_dict(), \"gan_generator.pkl\")\n",
    "torch.save(D.state_dict(), \"gan_discriminator.pkl\")\n",
    "with open('gan_train_history.pkl', 'wb') as f:\n",
    "    pickle.dump(train_hist, f)\n",
    "\n",
    "generated_image_array = [imageio.imread(generated_image) for generated_image in generated_images]\n",
    "imageio.mimsave(result_dir + '/GAN_generation.gif', generated_image_array, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_D(self):\n",
    "        \"\"\"\n",
    "        train Discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        # Real data\n",
    "        real = self.data.get_real_sample()\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "        generated_images = self.gan.G.predict(z)\n",
    "\n",
    "        # labeling and concat generated, real images\n",
    "        x = np.concatenate((real, generated_images), axis=0)\n",
    "        y = [0.9] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "        # train discriminator\n",
    "        self.gan.D.trainable = True\n",
    "        loss = self.gan.D.train_on_batch(x, y)\n",
    "        return loss\n",
    "\n",
    "def train_G(self):\n",
    "        \"\"\"\n",
    "        train Generator\n",
    "        \"\"\"\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "\n",
    "        # labeling\n",
    "        y = [1] * self.batch_size\n",
    "\n",
    "        # train generator\n",
    "        self.gan.D.trainable = False\n",
    "        loss = self.gan.GD.train_on_batch(z, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "\n",
    "class Data:\n",
    "    \"\"\"\n",
    "    Define dataset for training GAN\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, z_input_dim):\n",
    "        # load mnist dataset\n",
    "        # 이미지는 보통 -1~1 사이의 값으로 normalization : generator의 outputlayer를 tanh로\n",
    "        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "        self.x_data = ((X_train.astype(np.float32) - 127.5) / 127.5)\n",
    "        self.x_data = self.x_data.reshape((self.x_data.shape[0], 1) + self.x_data.shape[1:])\n",
    "        self.batch_size = batch_size\n",
    "        self.z_input_dim = z_input_dim\n",
    "\n",
    "    def get_real_sample(self):\n",
    "        \"\"\"\n",
    "        get real sample mnist images\n",
    "\n",
    "        :return: batch_size number of mnist image data\n",
    "        \"\"\"\n",
    "        return self.x_data[np.random.randint(0, self.x_data.shape[0], size=self.batch_size)]\n",
    "\n",
    "    def get_z_sample(self, sample_size):\n",
    "        \"\"\"\n",
    "        get z sample data\n",
    "\n",
    "        :return: random z data (batch_size, z_input_dim) size\n",
    "        \"\"\"\n",
    "        return np.random.uniform(-1.0, 1.0, (sample_size, self.z_input_dim))\n",
    "\n",
    "\n",
    "class GAN:\n",
    "    def __init__(self, learning_rate, z_input_dim):\n",
    "        \"\"\"\n",
    "        init params\n",
    "\n",
    "        :param learning_rate: learning rate of optimizer\n",
    "        :param z_input_dim: input dim of z\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.z_input_dim = z_input_dim\n",
    "        self.D = self.discriminator()\n",
    "        self.G = self.generator()\n",
    "        self.GD = self.combined()\n",
    "\n",
    "    def discriminator(self):\n",
    "        \"\"\"\n",
    "        define discriminator\n",
    "        \"\"\"\n",
    "        D = Sequential()\n",
    "        D.add(Conv2D(256, (5, 5),\n",
    "                     padding='same',\n",
    "                     input_shape=(1, 28, 28),\n",
    "                     kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Conv2D(512, (5, 5), padding='same'))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Flatten())\n",
    "        D.add(Dense(256))\n",
    "        D.add(LeakyReLU(0.2))\n",
    "        D.add(Dropout(0.3))\n",
    "        D.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        D.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        return D\n",
    "\n",
    "    def generator(self):\n",
    "        \"\"\"\n",
    "        define generator\n",
    "        \"\"\"\n",
    "        G = Sequential()\n",
    "        G.add(Dense(512, input_dim=self.z_input_dim))\n",
    "        G.add(LeakyReLU(0.2))\n",
    "        G.add(Dense(128 * 7 * 7))\n",
    "        G.add(LeakyReLU(0.2))\n",
    "        G.add(BatchNormalization())\n",
    "        G.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n",
    "        G.add(UpSampling2D(size=(2, 2)))\n",
    "        G.add(Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        G.add(UpSampling2D(size=(2, 2)))\n",
    "        G.add(Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        G.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        return G\n",
    "\n",
    "    def combined(self):\n",
    "        \"\"\"\n",
    "        defien combined gan model\n",
    "        \"\"\"\n",
    "        G, D = self.G, self.D\n",
    "        D.trainable = False\n",
    "        GD = Sequential()\n",
    "        GD.add(G)\n",
    "        GD.add(D)\n",
    "\n",
    "        adam = Adam(lr=self.learning_rate, beta_1=0.5)\n",
    "        GD.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        D.trainable = True\n",
    "        return GD\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, args):\n",
    "        self.epochs = args.epochs\n",
    "        self.batch_size = args.batch_size\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.z_input_dim = args.z_input_dim\n",
    "        self.data = Data(self.batch_size, self.z_input_dim)\n",
    "\n",
    "        # the reason why D, G differ in iter : Generator needs more training than Discriminator\n",
    "        self.n_iter_D = args.n_iter_D\n",
    "        self.n_iter_G = args.n_iter_G\n",
    "        self.gan = GAN(self.learning_rate, self.z_input_dim)\n",
    "        self.d_loss = []\n",
    "        self.g_loss = []\n",
    "\n",
    "        # print status\n",
    "        batch_count = self.data.x_data.shape[0] / self.batch_size\n",
    "        print('Epochs:', self.epochs)\n",
    "        print('Batch size:', self.batch_size)\n",
    "        print('Batches per epoch:', batch_count)\n",
    "        print('Learning rate:', self.learning_rate)\n",
    "        print('Image data format:', K.image_data_format())\n",
    "\n",
    "    def fit(self):\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # train discriminator by real data\n",
    "            dloss = 0\n",
    "            for iter in range(self.n_iter_D):\n",
    "                dloss = self.train_D()\n",
    "\n",
    "            # train GD by generated fake data\n",
    "            gloss = 0\n",
    "            for iter in range(self.n_iter_G):\n",
    "                gloss = self.train_G()\n",
    "\n",
    "            # print loss data\n",
    "            print('Discriminator loss:', str(dloss))\n",
    "            print('Generator loss:', str(gloss))\n",
    "\n",
    "    def train_D(self):\n",
    "        \"\"\"\n",
    "        train Discriminator\n",
    "        \"\"\"\n",
    "\n",
    "        # Real data\n",
    "        real = self.data.get_real_sample()\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "        generated_images = self.gan.G.predict(z)\n",
    "\n",
    "        # labeling and concat generated, real images\n",
    "        x = np.concatenate((real, generated_images), axis=0)\n",
    "        y = [0.9] * self.batch_size + [0] * self.batch_size\n",
    "\n",
    "        # train discriminator\n",
    "        self.gan.D.trainable = True\n",
    "        loss = self.gan.D.train_on_batch(x, y)\n",
    "        return loss\n",
    "\n",
    "    def train_G(self):\n",
    "        \"\"\"\n",
    "        train Generator\n",
    "        \"\"\"\n",
    "\n",
    "        # Generated data\n",
    "        z = self.data.get_z_sample(self.batch_size)\n",
    "\n",
    "        # labeling\n",
    "        y = [1] * self.batch_size\n",
    "\n",
    "        # train generator\n",
    "        self.gan.D.trainable = False\n",
    "        loss = self.gan.GD.train_on_batch(z, y)\n",
    "        return loss\n",
    "\n",
    "def main():\n",
    "    # set hyper parameters\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch_size', type=int, default=128,\n",
    "                        help='Batch size for networks')\n",
    "    parser.add_argument('--epochs', type=int, default=200,\n",
    "                        help='Epochs for the networks')\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.0002,\n",
    "                        help='Learning rate')\n",
    "    parser.add_argument('--z_input_dim', type=int, default=100,\n",
    "                        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_iter_D', type=int, default=1,\n",
    "                        help='training iteration for D')\n",
    "    parser.add_argument('--n_iter_G', type=int, default=5,\n",
    "                        help='training iteration for G')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # run model\n",
    "    model = Model(args)\n",
    "    model.fit()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
